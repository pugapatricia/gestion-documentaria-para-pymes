{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-KarOQNArIt"
      },
      "source": [
        "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pugapatricia/gestion-documentaria-para-pymes/blob/main/etiquetado/Generador_etiquetas.ipynb)\n",
        "\n",
        "[![Ver en GitHub](https://img.shields.io/badge/GitHub-Repo-black?logo=github)](https://github.com/pugapatricia/gestion-documentaria-para-pymes/tree/main/etiquetado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPOn6tibIKxm"
      },
      "source": [
        "#Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q_rAOUDjdz8D",
        "outputId": "20b68cfd-19da-4287-da2c-bbc1d44a43e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/117.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m112.6/117.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q requests msal python-docx PyPDF2 pandas openpyxl python-pptx openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fJ1Ko6i0cNaP"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import msal\n",
        "from docx import Document\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from pptx import Presentation\n",
        "import os\n",
        "import spacy\n",
        "import torch\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0E-lCnBdT9w"
      },
      "source": [
        "# Configuración"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = getpass.getpass(\"Introduce tu OpenAI API Key: \")\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "ukAIZ3q6q5zq",
        "outputId": "11eac251-29ba-4aff-98a5-9bc42437db55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Introduce tu OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "USm2IuiscQIt"
      },
      "outputs": [],
      "source": [
        "CLIENT_ID = \"e3f2393e-7348-47d1-9c64-8d8efe6a5e95\"\n",
        "AUTHORITY = \"https://login.microsoftonline.com/consumers\"\n",
        "SCOPE = [\"User.Read\", \"Files.ReadWrite\"]\n",
        "\n",
        "ext_permitidas = {\"pdf\", \"docx\", \"xlsx\", \"xls\", \"pptx\", \"txt\", \"csv\"}\n",
        "url = \"https://graph.microsoft.com/v1.0/me/drive/root:/Etiquetados:/children\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1uxqbv6eEsp"
      },
      "source": [
        "# Conección con OneDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSR-s8KvbuPf"
      },
      "outputs": [],
      "source": [
        "app = msal.PublicClientApplication(CLIENT_ID, authority=AUTHORITY)\n",
        "\n",
        "flow = app.initiate_device_flow(scopes=SCOPE)\n",
        "if \"user_code\" not in flow:\n",
        "    raise Exception(\"No se pudo iniciar el device flow. Revisa tu configuración en Azure.\")\n",
        "\n",
        "print(flow[\"message\"])  # 👉 Copia el código en https://microsoft.com/devicelogin\n",
        "result = app.acquire_token_by_device_flow(flow)\n",
        "\n",
        "if \"access_token\" not in result:\n",
        "    raise Exception(f\"Error autenticación: {result.get('error_description')}\")\n",
        "\n",
        "access_token = result[\"access_token\"]\n",
        "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
        "\n",
        "# Llamada a la API con tu token de acceso\n",
        "resp = requests.get(url, headers=headers)\n",
        "if resp.status_code != 200:\n",
        "    raise Exception(f\"Error al obtener archivos: {resp.text}\")\n",
        "data = resp.json()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_nDVZvpeU4D"
      },
      "source": [
        "# Funciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-wlPBMaFlMo"
      },
      "source": [
        "Procesamos cada documento de acuerdo con su tipo (PDF, Word, Excel, etc.) para convertirlo en datos estructurados. Este proceso se realiza mediante las siguientes funciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbiD5bBaeTgS"
      },
      "outputs": [],
      "source": [
        "def leer_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "def leer_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    return \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "\n",
        "def leer_excel(file_path):\n",
        "    try:\n",
        "        df_dict = pd.read_excel(file_path, sheet_name=None)\n",
        "    except Exception as e:\n",
        "        return f\"Error leyendo Excel: {e}\"\n",
        "\n",
        "    texto = []\n",
        "    for nombre, hoja in df_dict.items():\n",
        "        hoja = hoja.fillna(\"\")  # reemplaza NaN por \"\"\n",
        "        texto.append(f\"\\n--- Hoja: {nombre} ---\\n\")\n",
        "        texto.append(hoja.to_string())\n",
        "    return \"\\n\".join(texto)\n",
        "\n",
        "def leer_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df.to_string()\n",
        "\n",
        "def leer_pptx(file_path):\n",
        "    prs = Presentation(file_path)\n",
        "    texto = []\n",
        "    for i, slide in enumerate(prs.slides, 1):\n",
        "        texto.append(f\"\\n--- Diapositiva {i} ---\\n\")\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text_frame\") and shape.text_frame:\n",
        "                texto.append(shape.text_frame.text)\n",
        "    return \"\\n\".join(texto)\n",
        "\n",
        "def leer_txt(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def procesar_archivo_temporal(item):\n",
        "    nombre = item.get(\"name\")\n",
        "    ext = nombre.split(\".\")[-1].lower()\n",
        "    if ext not in ext_permitidas:\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3bq9a30FfQK"
      },
      "source": [
        "Función para solicitar a OpenAI la generación de etiquetas/tickers por documento, con un límite máximo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHqP9jJ3GFJI"
      },
      "outputs": [],
      "source": [
        "etiquetas_global = set()\n",
        "\n",
        "def sugerir_tickers(texto, max_etiquetas=10):\n",
        "    if not texto.strip():\n",
        "        return []\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Eres un asistente que recibe un texto de un documento.\n",
        "Devuelve solo las {max_etiquetas} palabras más importantes\n",
        "que podrían usarse como etiquetas del documento, una sola palabra cada una,\n",
        "en una lista separada por comas. No agregues explicaciones, solo las palabras.\n",
        "\n",
        "Texto:\n",
        "{texto}\n",
        "\"\"\"\n",
        "    try:\n",
        "        respuesta = client.chat.completions.create(\n",
        "            model=\"gpt-5-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "\n",
        "        etiquetas = respuesta.choices[0].message.content\n",
        "        etiquetas_lista = [e.strip().lower() for e in re.split(r'[,\\n;]+', etiquetas) if e.strip()]\n",
        "        etiquetas_lista = list(dict.fromkeys(etiquetas_lista))[:max_etiquetas]\n",
        "        etiquetas_global.update(etiquetas_lista)\n",
        "\n",
        "        return etiquetas_lista\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error al generar etiquetas: {e}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b55PVyzZGsXI"
      },
      "source": [
        "Descarga un archivo temporal desde una URL, verifica que su extensión esté permitida y lo lee según su tipo, devolviendo su contenido como texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XvATayYGtgq"
      },
      "outputs": [],
      "source": [
        "tickers_global = set()\n",
        "ext_permitidas = {\"pdf\", \"docx\", \"xlsx\", \"xls\", \"pptx\", \"txt\", \"csv\"}\n",
        "\n",
        "def procesar_archivo_temporal(item):\n",
        "    nombre = item.get(\"name\")\n",
        "    ext = nombre.split(\".\")[-1].lower()\n",
        "    if ext not in ext_permitidas:\n",
        "        return \"\"\n",
        "\n",
        "    tmp_path = f\"/tmp/{nombre}\"\n",
        "    try:\n",
        "        r = requests.get(item[\"@microsoft.graph.downloadUrl\"], stream=True)\n",
        "        r.raise_for_status()\n",
        "        with open(tmp_path, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        if ext == \"pdf\":\n",
        "            return leer_pdf(tmp_path)\n",
        "        elif ext == \"docx\":\n",
        "            return leer_docx(tmp_path)\n",
        "        elif ext in {\"xlsx\", \"xls\"}:\n",
        "            return leer_excel(tmp_path)\n",
        "        elif ext == \"pptx\":\n",
        "            return leer_pptx(tmp_path)\n",
        "        elif ext == \"txt\":\n",
        "            return leer_txt(tmp_path)\n",
        "        elif ext == \"csv\":\n",
        "            return leer_csv(tmp_path)\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"⚠️ Error descargando {nombre}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error procesando {nombre}: {e}\")\n",
        "    finally:\n",
        "        if os.path.exists(tmp_path):\n",
        "            os.remove(tmp_path)\n",
        "\n",
        "    return \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQmIJsSXerQS"
      },
      "source": [
        "# LISTAR Y PROCESAR ARCHIVOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t14O58CkepJq"
      },
      "outputs": [],
      "source": [
        "# Procesar los primeros 5 archivos no folders\n",
        "for idx, item in enumerate(data.get(\"value\", [])):\n",
        "    if idx >= 5 or \"folder\" in item:\n",
        "        continue\n",
        "\n",
        "    texto = procesar_archivo_temporal(item)\n",
        "    if texto.strip():\n",
        "        # Obtener sugerencias de tickers (empresas)\n",
        "        try:\n",
        "            sugerencias = sugerir_tickers(texto, max_etiquetas=10)\n",
        "            tickers_global.update([s.strip() for s in sugerencias if s.strip()])\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error generando etiquetas para {item.get('name')}: {e}\")\n",
        "\n",
        "print(\"\\nTickers finales:\\n\", sorted(tickers_global))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}